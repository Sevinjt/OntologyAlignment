{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355a241a",
   "metadata": {},
   "source": [
    "# Owl2vec_star_ext\n",
    "In this process, we get ontology files and to change it to embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d810e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sev_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO: 1\n",
      "INFO: \n",
      " Access the ontology ...\n",
      "INFO: There are 112354 triples in the ontology\n",
      "* Owlready2 * Creating new ontology omim <./case_studies/Data/omim.owl#>.\n",
      "* Owlready2 * ADD TRIPLE ./case_studies/Data/omim.owl http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Ontology\n",
      "* Owlready2 *     ...loading ontology omim from ./case_studies/Data/omim.owl...\n",
      "* OwlReady2 * Importing 116045 object triples from ontology ./case_studies/Data/omim.owl# ...\n",
      "* OwlReady2 * Importing 81649 data triples from ontology ./case_studies/Data/omim.owl# ...\n",
      "* Owlready2 *     ...8 properties found: IAO_0100001, RO_0002200, RO_0003303, consider, hasExactSynonym, hasSynonymType, exactMatch, category\n",
      "INFO: There are 310048 triples in the ontology\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 6.803836107254028 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.03472137451171875 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 36.723002672195435 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.03469061851501465 seconds \n",
      "INFO: \tExtracting triples associated to BFO_0000050\n",
      "INFO: \t\tTime extracting triples for property: 6.5475873947143555 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_317343\n",
      "INFO: \t\tTime extracting triples for property: 0.14071321487426758 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_317344\n",
      "INFO: \t\tTime extracting triples for property: 0.1719820499420166 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_317345\n",
      "INFO: \t\tTime extracting triples for property: 0.12427902221679688 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_317346\n",
      "INFO: \t\tTime extracting triples for property: 0.1439971923828125 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_317348\n",
      "INFO: \t\tTime extracting triples for property: 0.16312289237976074 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_317349\n",
      "INFO: \t\tTime extracting triples for property: 0.14913582801818848 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_327767\n",
      "INFO: \t\tTime extracting triples for property: 0.15020012855529785 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_410295\n",
      "INFO: \t\tTime extracting triples for property: 0.12482476234436035 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_410296\n",
      "INFO: \t\tTime extracting triples for property: 0.13791465759277344 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_465410\n",
      "INFO: \t\tTime extracting triples for property: 0.15656423568725586 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C016\n",
      "INFO: \t\tTime extracting triples for property: 0.7941262722015381 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C017\n",
      "INFO: \t\tTime extracting triples for property: 0.15785717964172363 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C020\n",
      "INFO: \t\tTime extracting triples for property: 0.13531994819641113 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C022\n",
      "INFO: \t\tTime extracting triples for property: 0.1475207805633545 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C025\n",
      "INFO: \t\tTime extracting triples for property: 0.1644279956817627 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C026\n",
      "INFO: \t\tTime extracting triples for property: 0.16020917892456055 seconds \n",
      "INFO: \tExtracting triples associated to Orphanet_C027\n",
      "INFO: \t\tTime extracting triples for property: 0.13794660568237305 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.2552340030670166 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 334.8500783443451 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 38.07333517074585 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 5.78218674659729 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.030945539474487305 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 34.37798285484314 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.022068262100219727 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.7251017093658447 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 38.13686537742615 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: \n",
      "\n",
      "\n",
      "Extraction time for ontologies:506.31\n",
      "* Owlready2 * Creating new ontology projection.ttl <./caches_omim2ordo/projection.ttl#>.\n",
      "* Owlready2 * ADD TRIPLE ./caches_omim2ordo/projection.ttl http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Ontology\n",
      "* Owlready2 *     ...loading ontology projection.ttl from ./caches_omim2ordo/projection.ttl...\n",
      "* OwlReady2 * Importing 28188 object triples from ontology ./caches_omim2ordo/projection.ttl# ...\n",
      "* OwlReady2 * Importing 60431 data triples from ontology ./caches_omim2ordo/projection.ttl# ...\n",
      "* Owlready2 *     ...0 properties found: \n",
      "INFO: There are 398668 triples in the ontology\n",
      "INFO: \n",
      "Calculate the ontology projection ...\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 8.2940673828125 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.048424720764160156 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 34.32806062698364 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.026849746704101562 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0011260509490966797 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.001004934310913086 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 75.93482422828674 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: Projection saved into turtle file: ./caches_omim2ordo/projection.ttl\n",
      "INFO: \n",
      "Extract classes and individuals ...\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 8.097676753997803 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.0394594669342041 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 33.796908378601074 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.02520585060119629 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.0 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 74.39375615119934 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: \n",
      "Extract axioms ...\n",
      "INFO: \n",
      "Extract annotations ...\n",
      "INFO: \n",
      "Generate URI document ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './case_studies/Data/omim2ordo2022.train+val_Int.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mowl2vec4mappings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Parameters:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ontology_file1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ontology_file2\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# mix_doc\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# but there are some parameters you can change in config file\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m gensim_model \u001b[38;5;241m=\u001b[39m \u001b[43mowl2vec4mappings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_owl2vec_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_file1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./case_studies/Data/ordo.owl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43montology_file2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./case_studies/Data/omim.owl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./case_studies/Data/omim2ordo2022.train+val_Int.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dfl2022.cfg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43muri_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmix_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec4mappings.py:45\u001b[0m, in \u001b[0;36mextract_owl2vec_model\u001b[1;34m(ontology_file1, ontology_file2, mapping_file, config_file, uri_doc, lit_doc, mix_doc)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     43\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 45\u001b[0m model_ \u001b[38;5;241m=\u001b[39m \u001b[43m__perform_ontology_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec4mappings.py:195\u001b[0m, in \u001b[0;36m__perform_ontology_embedding\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURI_Doc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURI_Doc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    193\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerate URI document ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m     walks_,instances \u001b[38;5;241m=\u001b[39m \u001b[43mget_rdf2vec_walks\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montology_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalker_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDOCUMENT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwalker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mwalk_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDOCUMENT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwalk_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m walks for \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m seed entities\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(walks_), \u001b[38;5;28mlen\u001b[39m(instances)))\n\u001b[0;32m    198\u001b[0m     walk_sentences \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m walks_]\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec_star\\lib\\RDF2Vec_Embed.py:201\u001b[0m, in \u001b[0;36mget_rdf2vec_walks\u001b[1;34m(ontology_file, mapping_file, walker_type, walk_depth, classes, config)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rdf2vec_walks\u001b[39m(ontology_file, mapping_file, walker_type, walk_depth, classes, config):\n\u001b[1;32m--> 201\u001b[0m     kg, walker, classes \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_kg_walker\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43montology_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mwalker_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwalker_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mwalk_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwalk_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m#kg.visualise()\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     instances \u001b[38;5;241m=\u001b[39m [rdflib\u001b[38;5;241m.\u001b[39mURIRef(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes]\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec_star\\lib\\RDF2Vec_Embed.py:143\u001b[0m, in \u001b[0;36mconstruct_kg_walker\u001b[1;34m(ontology_file, mapping_file, walker_type, walk_depth, config)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mapping_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     lines \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './case_studies/Data/omim2ordo2022.train+val_Int.tsv'"
     ]
    }
   ],
   "source": [
    "import owl2vec4mappings\n",
    "\n",
    "# Parameters:\n",
    "# ontology_file1\n",
    "# ontology_file2\n",
    "# mapping_file\n",
    "# config_file\n",
    "# uri_doc\n",
    "# lit_doc\n",
    "# mix_doc\n",
    "# but there are some parameters you can change in config file\n",
    "gensim_model = owl2vec4mappings.extract_owl2vec_model(ontology_file1 = \"./case_studies/Data/ordo.owl\",\n",
    "                                                ontology_file2 = './case_studies/Data/omim.owl',\n",
    "                                                mapping_file = './case_studies/Data/omim2ordo2022.train+val_Int.tsv',\n",
    "                                                config_file = \"./dfl2022.cfg\",\n",
    "                                                uri_doc = True, lit_doc = True, mix_doc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"dfl2022.cfg\")\n",
    "output_folder=config['DOCUMENT']['cache_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472dcc3-c29a-4c6b-bf7e-c58b88291500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim format\n",
    "#Run\n",
    "gensim_model.save(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73978c53-68cc-4b51-8419-e6908614459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Txt format\n",
    "#Run\n",
    "gensim_model.wv.save_word2vec_format(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a78df2",
   "metadata": {},
   "source": [
    "## Loading embeddings and getting similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8526b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "ranking_results = []\n",
    "# save the scored candidate mappings in the same format as the original test.cands.tsv\n",
    "f = open(output_folder+\"scored.test.cands\"+'_'+config['DOCUMENT']['walk_depth']+\".tsv\", \"w\")\n",
    "f.write(\"SrcEntity\" +'\\t'+ \"TgtEntity\" +'\\t'+ \"TgtCandidates\\n\")\n",
    "file = open(r\"/users/sbrt072/myfolder/OWL2Vec-Star/case_studies/Data/omim2ordo2022.test.cands.tsv\")\n",
    "\n",
    "for i in file.readlines()[1:]:\n",
    "    src_ref_class = i.split('\\t')[0]\n",
    "    tgt_ref_class = i.split('\\t')[1]\n",
    "    tgt_cands = i.split('\\t')[2]\n",
    "    # print(src_ref_class, tgt_ref_class, tgt_cands)\n",
    "    tgt_cands = eval(tgt_cands)  # transform string into list or sequence\n",
    "    scored_cands = []\n",
    "    for tgt_cand in tgt_cands:\n",
    "        # assign a score to each candidate with an OM system\n",
    "        try:\n",
    "            matching_score = model.wv.similarity(src_ref_class, tgt_cand)\n",
    "        except:\n",
    "            matching_score = 0\n",
    "        scored_cands.append([tgt_cand, matching_score])\n",
    "    ranking_results.append([src_ref_class, tgt_ref_class, scored_cands])\n",
    "    f.write(f'{src_ref_class}\\t{tgt_ref_class}\\t{scored_cands}\\n')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53ac11-4f6c-4bfa-be85-ede55e0f88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "import numpy as np\n",
    "distance_results = []\n",
    "# save the scored candidate mappings in the same format as the original test.cands.tsv\n",
    "f = open(output_folder+\"distance.test.cands\"+'_'+config['DOCUMENT']['walk_depth']+\".tsv\", \"w\")\n",
    "f.write(\"SrcEntity\" +'\\t'+ \"TgtEntity\" +'\\t'+ \"TgtCandidates\\n\")\n",
    "file = open(r\"/users/sbrt072/myfolder/OWL2Vec-Star/case_studies/Data/omim2ordo2022.test.cands.tsv\")\n",
    "\n",
    "\n",
    "for i in file.readlines()[1:]:\n",
    "    src_ref_class = i.split('\\t')[0]\n",
    "    tgt_ref_class = i.split('\\t')[1]\n",
    "    tgt_cands = i.split('\\t')[2]\n",
    "    # print(src_ref_class, tgt_ref_class, tgt_cands)\n",
    "    tgt_cands = eval(tgt_cands)  # transform string into list or sequence\n",
    "    scored_cands = []\n",
    "    for tgt_cand in tgt_cands:\n",
    "        # assign a score to each candidate with an OM system\n",
    "        try:\n",
    "            euc_dist = np.linalg.norm(model.wv[src_ref_class] - model.wv[tgt_cand])\n",
    "        except:\n",
    "            euc_dist = np.inf\n",
    "        scored_cands.append([tgt_cand, euc_dist])\n",
    "    distance_results.append([src_ref_class, tgt_ref_class, scored_cands])\n",
    "    f.write(f'{src_ref_class}\\t{tgt_ref_class}\\t{scored_cands}\\n')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d4764-08ea-4c6a-bf45-5d38352e0659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466cd7e-8432-4fe7-8074-0ccbdc187154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
