[BASIC]
# the file of input ontology; mandatory; you can also set it as a projected ontology
# ontology_file = ./case_studies/Data/ontology.owl

# ontology_file1 = ./case_studies/Data/university.owl
# ontology_file2 = ./case_studies/Data/CityUniversityofLondon.owl
# mapping = ./case_studies/Data/CityUniversityofLond_university.txt

# ontology_file = ./caches_cmt/projection.ttl
# ontology_file1 = ./case_studies/Data/cmt.owl
# ontology_file2 = ./case_studies/Data/Conference.owl
# mapping = ./case_studies/Data/ALIN-cmt-conference.txt

#ontology_file = ./caches_ncit2doid/projection.ttl
#ontology_file1 = ./case_studies/Data/ncit.owl
#ontology_file2 = ./case_studies/Data/doid.owl
#mapping = ./case_studies/Data/trainUnionNcit2doid.tsv

# ontology_file = ./caches_helis2foodon/projection.ttl
# ontology_file1 = ./case_studies/Data/helis_v1.00.owl
# ontology_file2 = ./case_studies/Data/foodon-merged.owl
# mapping = ./case_studies/Data/logmap_outputlogmap_anchors.rdf

#ontology_file = ./caches_s2nn2023/projection.ttl
#ontology_file1 = ./case_studies/Data/snomed.neoplas.owl
#ontology_file2 = ./case_studies/Data/ncit.neoplas.owl
#mapping = ./case_studies/Data/trainUnionS2NN.tsv

ontology_file = ./caches_s2np2023/projection.ttl
ontology_file1 = ./case_studies/Data/snomed.pharm.owl
ontology_file2 = ./case_studies/Data/ncit.pharm.owl
mapping = ./case_studies/Data/trainUnionS2Nph.tsv


# ontology_file = ./caches_ncit2doid/projection.ttl
# ontology_file1 = ./case_studies/Data/ncit.owl
# ontology_file2 = ./case_studies/Data/doid.owl
# mapping = ./case_studies/Data/ncit2doid.tsv

# Confidece values threshold for edges in mapping 
confidence_threshold = 0

# Seed entities will be entities mapping. Default: no
seed_entities_on_mapping = yes

#ontology_dir = ./case_studies/data/

[DOCUMENT]
# cache directory for storing files; if not set, it creates a default: ./cache/
# cache_dir = ./caches_myuniversity/
# cache_dir = ./caches_cmt/
cache_dir = ./caches_s2np2023/
# cache_dir = ./caches_helis2foodon/
# cache_dir = ./caches_snomed2ncit.neoplas/
#cache_dir = ./caches_ncit2doid/

# use or not use the projected ontology
# default: no
ontology_projection = yes

# Projection of only the taxonomy of the ontology without other relationships. Default: no
projection_only_taxonomy = no

#Using or not multiple labels/synonyms for the literal/mixed sentences. Default: no
multiple_labels = yes

#Avoid OWL constructs like rdfs:subclassof in the document. Default: no 
avoid_owl_constructs = no

#Document of sentences. Default: no
save_document = yes


# reasoning for extract axioms: hermit or elk or none; none denotes not using reasoning.
# axiom_reasoner = hermit
# axiom_reasoner = elk
axiom_reasoner = none

# the seed entities for generating the walks
# default: all the named classes and instances cached in cache/entities.txt
# comment it if the default python program is called to extract all classes and individuals as the seed entities
#pre_entity_file = ./caches_omim2ordo/entities.txt

# the annotations and axioms can pre-calculated and saved in the some directory (e.g., the cache directory)
# OWL2Vec will use the pre-calculated files if set, or it will extract them by default
# comment them if the default python program is called to extract annotations and axioms
# we should comment the following two lines before we run the system for the first time 82, 83, 76, and in 53 we change ontology_projection to yes. 
#pre_annotation_file = ./caches_omim2ordo/annotations.txt
#pre_axiom_file = ./caches_omim2ordo/axioms.txt

# walker and walk_depth must be set
# random walk or random walk with Weisfeiler-Lehman (wl) subtree kernel
walker = mapping4vec
walk_depth = 2

# use URI/Literal/Mixture document (yes or no)
# they can be over witten by the command line parameters
URI_Doc = yes
Lit_Doc = yes
Mix_Doc = no

# the type for generating the mixture document (all or random)
# works when Mix_Doc is set to yes
# Mix_Type = all
Mix_Type = random

[MODEL]

# the directory of the pre-trained word2vec model
# default: without pre-training
# comment it if no pre-training is needed
# pre_train_model = ~/w2v_model/enwiki_model/word2vec_gensim
# Available from https://tinyurl.com/word2vec-model

# the size for embedding
# it is set to the size of the pre-trained model if it is adopted
embed_size = 100

# number of iterations in training the word2vec model
# i.e., epochs in gensim 4.x.x
iteration = 70

# for training the word2vec model without pre-training
window = 5
min_count = 1
negative = 25
seed = 42

# epoch for fine-tuning the pre-trained model
epoch = 250
